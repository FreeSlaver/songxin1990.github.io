---
layout: page
title:  简历中怎么写项目经历？
category: resume
categoryStr: 简历
tags: [HowTo,项目描述]
keywords:
description:  true
---

对啊，现在找工作不也是一项磨难吗？跨过去了，又成长了。忍耐。

简历中关于【项目经历】的撰写方法，一定要按照我的“三步走”的思路去写：  
第一步：项目背景介绍，主要回答：为什么要做这个项目？  
第二步：工作/项目职责，主要回答：你是怎么做这个项目的？在整个项目中主要负责什么？  
第三步：项目成果，主要回答：项目做完之后的效果是什么？  
这个撰写框架其实也是跟STAR法则（情景、任务、行动、结果）有异曲同工之妙，


企业招聘的目的，是找到帮他解决问题的人，或者说，招聘帮他干活的人。如果你让他看到，你可以胜任他的工作，他就会想要你。
（最好是直接写会什么，然后写经历，证明自己会。）
企业也没有把握什么人能胜任，他只能假设，如果你掌握了工作所需要的几种核心技能，就是初步合格的人选。

所以，企业在简历上寻找的，就是你有没有他需要的那几种技能。这才是招聘人员最关心的信息。 所以，简历应该突出的就是，你拥有企业想要的技能，你的经历证明你可以胜任。

事实上，项目信息的写法有一个公式。
项目 = 产品 + 技术 + 结果

据说，谷歌要求应聘者描述经历时，每段经历必须提供下面三个信息：
做了什么产品
用到了什么技术
取得了什么结果
除了三个基本信息，项目描述还要注意下面几点。

（1）主要介绍新项目，你过去3年～4年的经历最关键。不要详细描述较旧的项目。

（2）突出项目规模，比如用户数量、数据有多少 TB、每天的收入金额或交易量。

（3）最好都用动词开头，这样让人感到简洁有力。为了避免单调，动词也可以适当变化，"开发"、"实现"、"部署"、"完成"这些词都可以换着用。

## 本人信息
- 姓名：宋鑫，性别：男，年龄：33，工作年限：10年
- 手机：18194089585，邮箱：xinsong666@foxmail.com
- 学校：长江大学，专业：计算机科学与技术
- 主页：https://javaclass.net，Github: https://github.com/FreeSlaver
- StackOverflow: https://stackoverflow.com/users/6708214/songxin

## 自我评价
- 10年以上自研项目开发经验，4年以上大型架构设计经验
- 在中集电商负责Kafka，Zipkin的技术引进，从0到1，应用场景落地，线上故障解决
  ，做过公司内部相关技术分享
- 具有分布式，高并发，高可用，高拓展，大数据处理的系统架构设计及研发经验
- 主导项目框架设计，改造升级，设计业务架构，数据模型，并推动项目落地
- 具有扎实的技术功底，对Kafka，Spring，Mybatis，Redis等开源项目均读过源码
- 主导的项目类型主要包括电商，供应链，物流，智慧企业，物联网等
- 对商业逻辑，行业，产品，业务，流量有一定的判断和认知

## 开源贡献
### 私有社工库
https://github.com/FreeSlaver/sgk
基于ELK技术栈的社工库实现，处理数据源323.9G加44.6G
### Maxwell-sink
https://github.com/FreeSlaver/maxwell-sink    
基于Kafka Connect和Maxwell开发的一个分布式的ETL数据同步工具
### Zipkin brave-sparkjava
https://github.com/openzipkin/brave    
这个PR是Sparkjava的zipkin实现，Sparkjava是一个Java MVC框架

## 掌握技能
- 熟练掌握IO/NIO，集合等基础框架
- 熟练掌握多线程编程，AQS，ReentrantLock，线程池等技术
- 熟练掌握JVM，垃圾回收器，垃圾回收算法
- 熟练掌握Spring，Spring Boot，Spring Cloud等技术
- 熟练掌握Redis，Kafka集群部署，监控，线上问题排查解决
- 熟练掌握Mybatis，Zookeeper，Dubbo，Zipkin编程
- 熟练使用Mysql，掌握事务，索引，查询优化
- 有微服务架构、领域驱动设计和RESTful API的设计经验
- 有在分布式/基于云的环境中工作的实际经验，有自己的私有云服务器
- 掌握分布式事务，分布式锁，分布式消息一致性实现
- 熟练掌握Docker，K8S等容器技术
- 熟练掌握Linux服务器命令操作，项目部署等
- 熟练使用Idea，SVN，Git，Maven，Gradle等工具
- 熟练阅读英文技术文档，读写能力良好
- 精通DevOps、现代构建策略、CI/CD、单元测试和自动化集成测试

## 工作经历
### 2021.05~2022.12　　　　货拉拉科技    　　　资深Java工程师
参与货拉拉物流系统的开发，负责运费微服务，物流微服务的开发，负责分布式日志系统与链路追踪系统的技术选型，架构设计，负责智慧运输项目中的ETL数据分析统计模块。
### 2019.03~2021.05					待业
父母感染冠状肺炎，辞职照料重症的父母，最终不幸双双去世，陷入长期抑郁
### 2016.05~2019.03　　　　丰巢科技    　　　　高级Java工程师
参与中集电商e栈后台系统重构，独立负责ESB消息总线服务，e栈全链路追踪服务，快递柜故障监控系统的架构设计，技术选型。被丰巢科技收购后，主要负责中集系统的维护，改造，接入到丰巢的工作，参与丰巢优惠商城的开发。
### 2012.06~2016.03　　　　中国安防    　　　　软件工程师
参与中国安防工资核算系统，多媒体信息发布系统，金蝶ERP系统二次开发，日志统一处理平台，短消息服务平台。

## 项目经验
### 货拉拉科技				智慧运输项目
**项目描述**：公司智能工厂产品中的一个智慧运输项目，智能工厂包括：物业管理，租户管理，智能楼宇管理，设备生命周期管理，车辆管理，订单管理等功能模块。
智慧运输项目，客户能够在后台下单进行货物运输，可实时监控车辆位置，运行轨迹，出入库时间等。同时能进行相关用户数据，订单数据，货物数据，车辆数据的统计分析。
**技术框架及实现**：使用Maxwell读取MySQL binlog将车辆表，用户表，订单表等同步到Kafka集群中。之后使用Spark Steaing消费Kafka中数据，实时存储到Hbase中。考虑到rowkey设计，设置压缩，实现数据均匀分区分布，解决了Hbase表的数据倾斜与数据热点等问题。
**岗位职责**  ：负责项目技术选型，架构设计，功能实现，技术文档输出，项目部署，维护等   项目所有功能代码实现，自测改Bug，项目部署，维护等Kafka各种相关问题，内在Bug解决，集群运行状态监控系统搭建

### 货拉拉科技				物流项目
**项目描述**：公司物流项目，用户可以直接在APP端，小程序端，网页端下订单，选择承运方式，填写发货方，收货方信息，货物信息，服务端实时计算出订单价格，用户点击支付生成订单并支付，拉货司机会上门区间，用户能实时查询订单状态及运行轨迹。
**技术框架及实现**：使用到了Spring Cloud，Nacos，Redis，Mybatis，xxl-job，Shiro，RocketMQ，ElasticSearch等，主要的微服务包括：订单，运费计算，支付，运力调度，物流信息，优惠券等微服务。
**岗位职责**  ：本人负责运费微服务，物流信息微服务的程序设计，开发，测试，API接口文档撰写等。

### 中集电商　　　　e栈核心业务服务切分
**项目描述**：之前发生过一次Redis集群崩溃，一次ActiveMQ集群崩溃，故障恢复花费了较长时间，对用户和投资方造成了恶劣影响，所以需要对e栈核心业务进行服务切分，来进行故障隔离，保证服务稳定性和高可用性。
为保障e栈服务的稳定性，需要进行服务切分，异地多活，将主从架构的MySQL服务进行切分。Maxwell-sink是基于Kafka Connect实现的分布式ETL数据多向同步工具，
e栈核心业务的异步多活实现，对集群服务进行水平扩展，按地区进行服务划分，进行故障隔离，

**技术框架及实现**：   
使用Maxwell读取MySQL binlog日志，主要是终端表，箱格表，包裹表等核心业务数据，按照终端所在区域进行切分，将数据同步到Kafka集群中，之后使用基于Kafka Connect开发的Maxwell-sink来消费，进行数据过滤，清晰，转换存储到多个MySQL实例中。同时也会进行双向写，保存一份全局完备数据，进行后续数据比对，恢复。项目二期，进行Redis集群，Kafka集群，dataserver，nameserver集群的多实例部署。这样发生故障后，可一件实时热切换，局部故障修复后，进行数据补齐，校验后，可进行服务切换。

### 岗位职责
- 负责项目技术选型，架构设计，功能实现，技术文档输出，项目部署，维护；Kafka各种相关问题，内在Bug解决，集群运行状态监控系统搭建

### 中集电商  		快递柜终端智能监控分析系统
**项目描述**： 收集3万多台快递柜终端上传到12台服务器上的日志数据，对指定终端，箱格，包裹按天周月的时间维度进行数据统计分析，输出报表，对运营，运维的工作提供数据支持。 同时还能监控并自动发现故障的终端，主动告警并提交工单系统，提高运维的工作效率和用户体验。   
**技术框架及实现**：使用Logstash收集操作流水日志，业务日志等，传输到Kafka之后进行消费，对通过故障判断条件验证终端，比如5分钟没心跳，流水日志id落后过多等自动发现故障终端，推送到运维工单系统。同时对终端，箱格，包裹等按天周月的时间维度进行实时数据统计，将结果保存到ES中，用Kibana做报表分析。
**个人负责**：日志平台和智能化运维系统的技术选型，架构设计，功能模块开发，Kafka，ES集群的搭建，维护，技术文档撰写，生产环境问题追踪，定位解决，并写报告分析故障产生原因及解决方案 等 















